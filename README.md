# **Angles Don’t Lie: Unlocking Training-Efficient RL Through the Model’s Own Signals**



## Abstract

Current Reinforcement Fine-tuning (RFT) paradigms for Large Language Models (LLMs) suffer from sample inefficiency due to the redundant exposure of identical queries under uniform data sampling. While previous work has explored curriculum learning via heuristic difficulty metrics, *these strategies exhibit limitations by neglecting the intrinsic learning signals generated by the model itself, thus leading to suboptimal training regimes.* In this paper, we identify **a model-inherent signal termed angle concentration that effectively reflects an LLM's capacity to learn from specific data.** We theoretically and empirically demonstrate a correlation between the angular distribution of token hidden state vectors and the resulting gradient, revealing a learning preference for data exhibiting higher angle concentration. Inspired by this finding, we propose **GAIN-RL, a Gradient-driven Angle-Informed Navigated RL framework.** By leveraging the model's intrinsic angle concentration signal, GAIN-RL dynamically selects training data in each epoch, ensuring consistently impactful gradient updates and thus significantly enhancing overall training efficiency. Empirical evaluations show that GAIN-RL (GRPO) achieves over a 2.5X acceleration in training efficiency across diverse mathematical and coding tasks and varying model scales. Furthermore, GAIN-RL (GRPO)'s efficient sampling yields data-efficient training, achieving better performance with half the original data compared to vanilla GRPO with full training data.

**Paper Link:** 



![Overview](images/overview.png)**Overview of GAIN-RL.** GAIN-RL consists of three steps: *(1) Angle-based Data Reordering*: Before training, the model pre-fills all data and ranks them by the combined angle concentration signals: *C*inter + *C*intra. *(2) Gaussian-based Data Sampling*: During training, each epoch begins by sampling reordered data using a Gaussian distribution. *(3) Dynamic Probability Update*: Epoch-wise accuracy and angle concentration are collected to dynamically update *µt*+1. GAIN-RL guides the model to focus on high-angle, high-loss data, promoting effective gradients and faster convergence.



**The current release version includes:**

✅  **Integrated GAIN-RL into [verl](https://github.com/volcengine/verl/tree/main):**  We implemented GAINRL on `verl`. You can enable GAINRL simply by adding `data.gainrl.enable=enable` to the command line.

✅  **Integrated GAIN-RL into [trl](https://github.com/huggingface/trl):**  We provide GAINRL training script using `transformers.Trainer`  and `trl`. You can now easily use GAIN-RL and make changes.

✅  **Comprehensive performance evaluation:** We provide evaluation scripts for different datasets in *evaluation/*, taking into account the characteristics of different datasets.



## Step 1: Data Processing

Before training begins, GAINRL needs to sort the training data based on angular concentration according to the model being trained. This step only involves pre-filling stage, so it is fast (approximately 10 minutes). Only basic libraries including `transformers` and `datasets` are required to run this step.

We provide the dataset JSON files (including GSM8K, MATH, AMC, DEEPSCALER) used in the paper under the `dataset/` directory. Alternatively, you can manually place your own dataset files in the `dataset/` folder. Run the following steps to process the data.

```python
python data_processor.py 
--model_name [HUGGINGFACE_URL/LOCAL_PATH OF MODEL] 
--dataset_path [PATH OF DATASET]
--save_path [PATH OF PROCESSED DATA]
--gpu_id [ID OF UDED GAPU]
```

Example:

```python
python data_processor.py 
--model_name Qwen/Qwen2.5-0.5B-Instruct 
--dataset_path dataset/math_train.json 
--save_path angle_sorted_data/qwen2_5_05b_Instruct/processed_math.pt 
--gpu_id 1
```

The processed data will be saved under the `angle_sorted_data/` directory. **We also provide a subset of preprocessed data in `angle_sorted_data/`, which you can use for training directly.**



## Step 2: Training with GAINRL

We provide two options for training: **`verl`** and **`trl` + `transformers.Trainer`**. You can choose the framework you're more familiar with. In general, `verl` is better suited for efficient large-scale training, while `transformers.Trainer` with `trl` offers a simpler interface for modifying the code.

### [Training on `verl`]

**1. Environment Setup:** Navigate to the directory `cd verl_gainrl`, and follow the instructions in the [install doc](https://verl.readthedocs.io/en/latest/start/install.html) to set up the environment.

**2. Training with GAINRL:** We provide training scripts under `verl_gainrl/examples/gainrl_grpo/`. You can easily start training by using these scripts. You can also enable GAINRL by setting the following flags in your training command:

```python
python3 -m verl.trainer.main_ppo \
    ... \
    data.gainrl.enable: Ture
    data.gainrl.n: [STEP SIZE]
    data.gainrl.beta: [TARGET ACC]
    data.gainrl.alpha: [SENSITIVITY TO ANGLE]
    data.gainrl.adj_max: [MAX STEP]
    data.gainrl.adj_min: [MIN STEP]
    data.gainrl.processed_file: [PATH TO PROCESSED DATA]
    ...
```

 Example:

```python
python3 -m verl.trainer.main_ppo \
    ... \
    data.gainrl.enable: Ture
    data.gainrl.n: 500
    data.gainrl.beta: 0.5
    data.gainrl.alpha: 2
    data.gainrl.adj_max: 1000
    data.gainrl.adj_min: 0
    data.gainrl.processed_file: ../angle_sorted_data/qwen2_5_05b_Instruct/processed_gsm8k.pt
    ...
```



### [Training on `trl` + `transformers.Trainer`]

**1. Environment Setup:** Navigate to the trl directory and install the required environment.

```
cd trl_gainrl
conda create -yn gainrl python=3.10
conda activate gainrl
pip install -e .
cd ..
```

**2. Training with GAINRL:** We provide simple training scripts to help you get started easily. You can start training using the following command:

```python
python trl_gainrl/gainrl_trainer.py \
--dataset [DATASET NAME] \
--dataset_path [DATASET PATH] \
--model_name [MODEL PATH] \
--indices_path [PROCESSED DATA PATH] \
--gpu_id [GPU ID] \
--output_dir [OUTPUT DIRECTORY] \
--learning_rate [LEARNING RATE] \
--batch_size [BATCH SIZE] \
--num_epochs [NUMBER OF EPOCHS] \
--subset_size [SUBSET SIZE] \
--max_prompt_len [MAX PROMPT LENGTH] \
--max_completion_len [MAX COMPLETION LENGTH] \
--num_generations [NUM GENERATIONS PER SAMPLE] \
--gradient_accumulation_steps [GRADIENT ACCUMULATION STEPS] \
--save_steps [SAVE STEPS] \
--logging_steps [LOGGING STEPS]
```

Example: 

```python
python trl_gainrl/gainrl_trainer.py \
--dataset math \
--dataset_path ./dataset/math_train.json \
--model_name Qwen/Qwen2.5-0.5B-Instruct \
--indices_path ./angle_sorted_data/qwen2_5_05b_Instruct/processed_math.pt \
--gpu_id 1
```

By default, the trained model will be saved to the `trl_gainrl/output/` directory.



## Step 3: Evaluation

We provide evaluation scripts in the `evaluation/` directory, each tailored to different datasets based on their specific answer formats (e.g., integers, fractions, etc.). You can run evaluation using the following command:

```Shell
python evaluation/eval_[TASK].py \
--model_path [MODEL PATH] \
--dataset_path [DATASET PATH] \
--batch_size [BATCH SIZE]
```

Example：

```Shell
python evaluation/eval_gsm8k.py \
--model_path Qwen/Qwen2.5-0.5B-Instruct \
--dataset_path dataset/gsm8k_eval.json \
--batch_size 8
```



## Paper and Citation

More technical details can be found in our paper. If you find GAINRL useful or relevant to your project and research, please kindly cite our paper:

```

```



